# 9장: 웹 로봇

## 웹 로봇이란

- 웹 로봇 크롤러는 웹을 돌아다니며 HTML 문서를 검색해 각 페이지안의 URL 링크를 파싱하여 크롤링 페이지들의 목록에 추가

![웹로봇-루트집합.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e52be986-6e70-43c6-9ebb-481154a716d2/%E1%84%8B%E1%85%B0%E1%86%B8%E1%84%85%E1%85%A9%E1%84%87%E1%85%A9%E1%86%BA-%E1%84%85%E1%85%AE%E1%84%90%E1%85%B3%E1%84%8C%E1%85%B5%E1%86%B8%E1%84%92%E1%85%A1%E1%86%B8.png)

- 루트 집합
  - 웹 로봇 크롤러가 방문을 시작하는 URL들의 초기집합
  - 웹 로봇은 루트 집합을 잘 잡아야 광범위한 웹 리소스를 수집할 수 있음
- 웹 로봇은 **루프나 순환**에 빠지지 않도록 매우 조심해야 함

```basic
A => B => C => A => B => C
```

### 순환을 예방하는 법

- URL 정규화
  - 정규화를 통해 표준형식으로서 웹 로봇이 재방문할 리소스를 미리 제거
- 너비 우선 크롤링
- 스로틀링
  - 일정 시간 내에 웹로봇이 가져올 수 있는 페이지의 개수를 제한
- URL 크기 제한
  - 크기가가 너무 큰 경우(1KB) 크롤링 제한
  - URL 길이가 길다는 것은 반대로 순환을 하고 있다는 증거가 될 수 있음
  - 그러나 반드시 그런 것만은 아니며 이로 인해 가져올 수 있는 정보가 제한적일 수 있음
- URL/사이트 블랙리스트
- 패턴 발견
- 콘텐츠 지문(fingerprint)
  - 콘텐츠에서 일부 몇 바이트를 얻어 체크섬을 계산
  - 일치하는 문서가 있는 경우, 크롤링 하지 않음
- 사람의 관리 감독
  - 완벽한 웹로봇은 없다
  - 관리 감독, 로깅을 통해 잘못된 순환의 길을 막도록 늘 모니터링은 필수

## 로봇의 HTTP

- 로봇의 신원, 능력, 출신을 가늠할 수 있는 식별자를 크롤링하는 리소스에 전달하는 것을 로봇 개발자에게는 권장 사항

| HEADER     | CONTENT                                                             |
| ---------- | ------------------------------------------------------------------- |
| User-Agent | 로봇의 이름                                                         |
| From       | 로봇 사용, 관리자 이름 혹은 이메일                                  |
| Refer      | 레퍼런스 즉, 현재의 요청 URL을 포함한 해당 리소스의 부모 문서의 URL |

- 조건부 요청
  - 로봇 중 몇 몇은 시간이나 엔터티 태그를 비교해서 그들이 받아간 마지막 버전 이후에 업데이트 된 것이 있는지 알아보는 조건부 HTTP 요청을 구현
- 응답 다루기
  - 로봇은 GET 요청을 이용해 콘텐츠를 찾아 내지만 웹 탐색, 상호작용을 좀 더 잘해보려는 일부 웹 로봇들은 HTTP 응답을 다룰 줄 안다. (혹은 그래야만 한다.)

## 웹 로봇 차단

### robots.txt

- 웹 서버는 서버의 문서 루트에 `robots.txt` 라는 이름의 선택적 파일을 제공
- 이 파일은 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보 즉 규칙이 담겨있음

### robots.txt 파일 가져오기

- 웹 서버에 HTTP GET 요청을 통해 가져옴
- 요청 예시

```basic
GET /robots.txt HTTP/1.0
Host: www.gemma-good-software.com
User-Agent: GemGem/1.0
...
```

- 웹 서버의 응답 값에 따라 웹로봇은 다르게 동작
  - 200 ⇒ robots.txt 존재 ⇒ 규칙에 따라 크롤링
  - 404 ⇒ robots.txt 존재하지 않음 ⇒ 제약 없이 크롤링
  - 401, 403 ⇒ 제한 ⇒ 크롤링 불가
  - 503 ⇒ 일시적 실패 ⇒ 크롤링 뒤로 미룸
  - 3XX ⇒ 리다이렉트 ⇒ 리소스를 발견할 때까지 다이렉트를 따라감

### 로봇 제어 META 태그

- 웹 서버 개발자는 웹 로봇을 제어하기 위해 HTML 문서에 직접 태그를 추가

```basic
<META NAME="{WEB_ROBOT_NAME}" CONTENT={TAG1},{TAG2},{TAG3},{TAG4}>
```

| 태그      | 의미                                            |
| --------- | ----------------------------------------------- |
| NONE      | NOINDEX, NOFOLLOEW                              |
| ALL       | INDEX, FOLLOW                                   |
| NOINDEX   | 페이지 리소스 무시                              |
| INDEX     | 색인 허용                                       |
| NOFOLLOW  | 이 페이지가 링크한 페이지 크롤링 불가           |
| FOLLOW    | 이 페이지가 링크한 페이지 크롤링 허용           |
| NOARCHIVE | 캐시를 위한 로컬 사본을 만들어서 안 된다고 전달 |

## 검색 엔진

### 아키텍쳐

- 검색엔진 웹 크롤러들은 웹 리소스들을 수집
- 풀 텍스트 색인(full text index) 데이터베이스에 추가
  - 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스

### 질의 보내기

![웹로봇-질의보내기.png](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ae7715c6-5451-42e7-823f-035a173397ea/%E1%84%8B%E1%85%B0%E1%86%B8%E1%84%85%E1%85%A9%E1%84%87%E1%85%A9%E1%86%BA-%E1%84%8C%E1%85%B5%E1%86%AF%E1%84%8B%E1%85%B4%E1%84%87%E1%85%A9%E1%84%82%E1%85%A2%E1%84%80%E1%85%B5.png)

- 사용자는 `다람쥐` 라는 검색해 브라우저는 매개변수를 URL의 일부로 포함하는 GET 요청으로 변경해 요청
- 웹 서버들은 이 요청을 받아 검색 브라우저에게 넘겨준 뒤, 이를 사용자를 위한 HTML 페이지로 변환해 정렬

### 스푸밍

- 웹 로봇을 차단해 회원의 개인정보나 결제정보를 웹 로봇에게 노출되지 않게 하려는 반면, 웹 로봇에게 많이 노출되어 마케팅에 활요하려는 상업적 움직임도 존재
- 검색 엔진 웹 로봇을 속이지기 위한 해당 페이지와 관련된 수많은 키워드들을 나열한 가짜 페이지를 생성하는 애플리케이션도 개발

### 참고자료

- https://watrv41.gitbook.io/devbook/web/http/9
- https://azderica.github.io/til/docs/web/http-perfect-guide/ch9/
- https://velog.io/@200ok/HTTP-웹-로봇
